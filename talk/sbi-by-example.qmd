---
title: "Probabilistic Inversion of a PDE-Solver"
subtitle: "Conditioning a complex stochastic spatial simulator on observations"
date: 2025-01-22
author:
  - name: "Hans Olischläger"
    email: hans@olischlaeger.com
    orcid: 0009-0002-1285-2959
    affiliation: Universität Heidelberg
title-slide-attributes:
  data-notes:
    "
I'm Hans, from a little down the Neckar in Heidelberg. Nice to meet you.

Since I work on simulation-based inference, I thought it would be most useful for everybody when I basically explain what that even is in general and step through an example application.

So you will mainly hear about how you can condition a PDE-Solver on observations with SBI, and I'll briefly mention some methodic research I have been involved in in the end.
    "
format:
  clean-revealjs:
    slide-number: true
    footer: "Hans Olischläger -- Probabilistic Inversion of a PDE-Solver"
bibliography: references.bib
fig-align: "left"
---


::: {.callout-tip}

## Abstract

Simulation-Based Inference (SBI) is an established method and line of methodological research.

In this talk I show some of my work on SBI and approach it by looking on a particular scientific application: solving the inverse problem of pattern formation, i.e. which model parameters are likely to have resulted in a given observed pattern.

Thus, we will progress from a conceptual to a mathematical perspective and finally to an implementation. The challenges we face on this journey are twofold: solving a PDE reliably and fast to get good and enough training data, and data-efficient training of (generative) neural networks tasked with the inverse problem.

:::

# Simulation-based inference (SBI)

<!-- @cranmer2020; @tejero-cantero2020sbi; @radev2020 -->

::: {.notes}
So let's do a quick intro to SBI so we are all on the same page.

I am not telling you anything new, when I say that simulations are nowadays central to the scientific method.
You just can't get around writing highly complex code for testing your highly complex hypothesis.
In fact, in some fields you could even say, that research *IS* the effort of improving simulators.
Then, simulation code is a kind of concrete condensate of the scientific theories.

Climate models are a good example for this - they are our best guesses
of how each of the compartments ocean, atmosphere, land, ... work internally and also how they interact.

But to build better simulators you obviously need to not only simulate them, but also evaluate them against observations.
The most principled way to do this is by statistical inference.
Now, simulation-based inference is concerned with making statistical inference tools that can keep up with the needs of such simulators.

Traditional approaches to statistical inference need the likelihood of the simulator, but that typically not available.


define SBI


add refs: cranmer
:::


##
With *any* black-box simulator $\mathcal S_\theta$ you can:
$$
\theta \sim p(\theta)
$$

::: {.fragment .fade-in}
$$
x \sim p(x|\theta)
$$
✅ sample from the joint probability distribution
$(\theta, x) \sim p(x, \theta)$
<!-- = p(x|\theta) p(\theta)$ -->

:::


::: {.fragment .fade-in}
❌ evaluate likelihood $p(x|\theta)$

❌ evaluate or sample from posterior $p(\theta|x)$
:::

::: {.notes}
A simulator will in general have some parameters of interest theta and can generate corresponding data x

So by saying these parameters and data are samples of a structured probability distribution, we can say:

we sampled (theta, x) from the joint probability distribution.


--

What we cannot do is evaluate these distributions, and specifically not the most interesting one:

the probability of some parameters given an observation, called the posterior.
:::

<!-- ## -->
<!---->
<!-- ✅ sample from the joint probability distribution -->
<!-- $(\theta, x) \sim p(x, \theta)$ -->
<!---->
<!-- ❌ evaluate likelihood $p(x|\theta)$ -->
<!---->
<!-- ❌ evaluate or sample from posterior $p(\theta|x)$ -->
<!---->
<!-- ## -->
<!---->
<!-- ✅ sample from the joint probability distribution -->
<!-- $(\theta, x) \sim p(x, \theta)$ -->
<!---->
<!-- ❌evaluate likelihood $p(x|\theta)$ -->
<!---->
<!-- ✅ evaluate or sample from posterior $p(\theta|x)$ -->
<!---->
::: {.notes}
As you know we can train generative NNs from samples, for example with the NLL-loss.

These models become approximations of the distribution that is implicitly defined by the samples.

With score-matching, coupling flows, free-form-flows, consistency models, ... ? we can not only generate more samples from these distributions, but also evaluate the densities.
:::

## Simulation-based inference (SBI) ^[@cranmer2020; @tejero-cantero2020sbi; @radev2020]  {visibility="uncounted" auto-animate=true}

Parameterize distribution <span style="color:grey;font-size: 60%;">(e.g. coupling flow, score-matching, free-form-flow, consistency model, transformer, ...)

optimize a *proper scoring rule*^[@gneiting2007], like log-score ($\equiv$ NLL $\rightarrow$ $D_{KL}$)

✅ likelihood $p(x|\theta)$, posterior $p(\theta|x)$, evidence $p(x)$

✅ functionals of all the above

## Many different approaches
![Overview by @cranmer2020](figures/sbi-schemes.png){width=70%}

::: {.notes}
many approaches. This is an overview slide from Cranmer et al. And this is also the paper I would recommend you to read first.

We focus on approximating the full posterior distribution with a generate neural network.
:::

## Many different approaches {visibility="uncounted"}

![Overview by @cranmer2020](figures/sbi-schemes-focus.png){width=70%}

## Many different approaches {visibility="uncounted"}

![Overview by @cranmer2020](figures/sbi-schemes-focus-zoom.png){width=70%}


## My research in SBI

* Global sensitivity analysis ^[ published; @elsemuller2024]
  <!-- ![](figures/climate_sensitivity_ridge_plot_1.5_2023_SSP2.png){width=25%} -->
<!-- * ground truth for inverse kinematics benchmark ^[code on <https://codeberg.org/han-ol/InverseKinematicsSBI>] -->
* Point estimation techniques <!-- {.fragment .semi-fade-out} -->
* Application: Pattern formation

## Turing Patterns: Gierer-Meinhardt

PDE describing the reaction and diffusion of two chemicals


$$
\tiny\begin{split}
\partial_t u_1 =& \; a - b u_1 + \frac{u_1 ^2} {u_2 (1+c u_1^2)} &+ s \Delta u_1 \\
\partial_t u_2 =& \; u_1 ^2 - u_2 &+ \delta s \Delta u_2 \\
0 <& \; a,b,c,\delta
\end{split}
\normalsize
$$

spontaneous pattern formation from uniform noise
{{< embed ../notebooks/computations-for-talk.ipynb#fig-patterns-prior-predictive-few >}}

::: {.notes}
It is an example system showing diffusion driven (Turing) instability and consists of two chemicals that can react and have a different diffusion constant.

Given an observation, determine all the model configurations that are able to produce it. In other words, what is the posterior probability $p(\theta|x_{obs})$ of all model configurations $\theta=(a, b, c, \delta)$ given some (set of) experimental data $x_{obs}$.
:::

## Prior
::: {.notes}
If we don't have any other information than the parameters are positive, the Bayesian way is to choose something uninformative, like a ...
It turns out, that the Gierer-Meinhardt model only has this famous Turing instability for some parameters.
So what I did instead is, I sample proposals from a uniform distribution and reject anything that is bound to produce just a homogeneous state.

The prior is a distribution on a 4-dimensional support. But we can look at the marginal distributions and all pairwise relationships.
:::

{{< embed ../notebooks/computations-for-talk.ipynb#fig-parameters-prior >}}


## Simulator {.smaller}

* randomly perturbed homogeneous initial conditions
* periodic boundary conditions


::: {.fragment .fade-in}
* Solve the parameterized PDE until fixed point
$$
\tiny\begin{split}
\partial_t u_1 =& a - b u_1 + \frac{u_1 ^2} {u_2 (1+c u_1^2)} &+ s \Delta u_1 \\
\partial_t u_2 =& u_1 ^2 - u_2 &+ \delta s \Delta u_2
\end{split}
\normalsize
$$

:::

::: {.fragment .fade-in}
{{< embed ../notebooks/computations-for-talk.ipynb#fig-patterns-prior-predictive-few >}}
:::

<!-- {{< embed ../notebooks/computations-for-talk.ipynb#fig-patterns-prior-predictive-many >}} -->

::: {.notes}

:::

## Inference for observation A

{{< embed ../notebooks/computations-for-talk.ipynb#fig-obs-spot-strip-mix >}}


## Inference for observation A {visibility="uncounted"}
:::: {.columns}

::: {.column width="24%"}
{{< embed ../notebooks/computations-for-talk.ipynb#fig-obs-spot-strip-mix >}}
:::


::: {.column width="60%"}
{{< embed ../notebooks/computations-for-talk.ipynb#fig-inference >}}
:::

::::

## Inference for observation A {visibility="uncounted"}
:::: {.columns}

::: {.column width="24%"}
{{< embed ../notebooks/computations-for-talk.ipynb#fig-obs-spot-strip-mix >}}
:::


::: {.column width="65%"}
{{< embed ../notebooks/computations-for-talk.ipynb#fig-resim-spot-strip-mix >}}
:::

::::

## Inference for observation B

{{< embed ../notebooks/computations-for-talk.ipynb#fig-obs-laby >}}

## Inference for observation B {visibility="uncounted"}
:::: {.columns}

::: {.column width="24%"}
{{< embed ../notebooks/computations-for-talk.ipynb#fig-obs-laby >}}
:::

::: {.column width="60%"}
{{< embed ../notebooks/computations-for-talk.ipynb#fig-inference-laby >}}
:::

::::

## Inference for observation B {visibility="uncounted"}
:::: {.columns}

::: {.column width="24%"}
{{< embed ../notebooks/computations-for-talk.ipynb#fig-obs-laby >}}
:::

::: {.column width="65%"}
{{< embed ../notebooks/computations-for-talk.ipynb#fig-resim-laby >}}
:::

::::


## Diagnostics {.smaller}

{{< embed ../notebooks/computations-for-talk.ipynb#fig-recovery >}}
{{< embed ../notebooks/computations-for-talk.ipynb#fig-cal >}}

::: {.notes}
In the upper plot each dot shows the ground truth parameter for some observation against the approximated posterior distribution.

With the lower plots we can judge whether the uncertainty of the model is *calibrated*.
A calibrated model is one where the empirical cumulative distribution function of the approximated posterior matches the quantile level of the true parameter.
The most sensitive test is on the right and we can see that the lines for each of the four parameters are firmly inside these circular confidence bands.
:::

<!---->
<!-- ## Inverse kinematics -->
<!-- ![Figure 14: "Joint" samples](figures/IK-training-dataset-standard.png){width=40%} -->
<!---->
<!-- ## Inverse kinematics -->
<!-- ![Figure 15: Approximate conditioning with generative neural network](figures/IK-central-posterior.png){width=40%} -->
<!---->
<!---->
<!-- ## Inverse kinematics -->
<!-- ![Figure 16: Example of exact target conditional distribution ^[code on <https://codeberg.org/han-ol/InverseKinematicsSBI>]](figures/IK-central-posterior-exact.png){width=40%} -->
<!---->
<!-- ## Inverse kinematics -->
<!-- ![Figure 16: Showcasing fast sampling for benchmarking ^[code on <https://codeberg.org/han-ol/InverseKinematicsSBI>]](figures/IK-animation.gif){width=40%} -->
<!---->

## Global Sensitivity Analysis
![Figure 11: Sensitivity overview ^[@elsemuller2024]](figures/sensitivity_main_figure.png)

::: {.notes}
Sensitivity analysis is concerned with understanding the influences of all the little design decisions you do;
that is which prior, likelihood, network architecture and preprocessing you chose.

So we introduced a general purpose approach to study global sensitivity that can be used regardless of which scoring rule you choose.

It can be understood as doing all the analysis separately, but in parallel - to be more efficient.
So it is basically a workflow innovation.
:::

## Sensitivity in climate forecasts
![Figure 12: Simulated temperatures in different trajectories and unseen observation for 2023 ^[@elsemuller2024]](figures/temp-map.png)

## Sensitivity in climate forecasts {visibility="uncounted"}
![Figure 12: Simulated temperatures in different trajectories and unseen observation for 2023 ^[@elsemuller2024]](figures/temp-map-focus.png)

## Sensitivity in climate forecasts
![Figure 13: Predicted years of exceeding 1.5°C threshold ^[@elsemuller2024]](figures/climate_sensitivity_ridge_plot_1.5_2023-1.png){width=55%}

## Toolbox Development
::::: {.columns}

:::: {.column width="80%"}
Science needs good toolboxes to apply the methods we develop

::: {.incremental}
* convenient
* flexible
* diagnosable
* trustworthy
* well-documented
:::
::::

:::::::::::: {.column width="20%"}
![](figures/sbi-logo.png)

![](figures/bf-logo.png)
::::

:::::



## Thanks to my collaborators!
![](figures/group-pic.jpg){width=60%}

Computer Vision and Learning Group - IWR @ Uni Heidelberg

& Lasse Elsemüller, Stefan Radev, Paul Bürkner, Ayush Bharti

# Any questions?

# References
